# -*- coding: utf-8 -*-
"""Ensemble Models 3D GPU.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14DYSE7EmYKTGNAi3tORcOG5c4XwbKQJY
"""

import numpy as np
import os
import gzip, shutil
import nibabel as nib
import time
import random

import torch
from torch.utils.data import Dataset

import torch.nn.functional as F
from torch import nn as nn
from torch.autograd import Variable
from torch.nn import MSELoss, SmoothL1Loss, L1Loss

nobackup = '/nobackup/sc19rw/Train/'
nobackup_models = '/nobackup/sc19rw/Models/'
home = '/home/home01/sc19rw/'

MRI_ids = np.load(home+"MRI_ids.npz") #make sure you use the .npz!
MRI_ids = MRI_ids['arr_0']

import pandas as pd
import random


root = nobackup_models

data = {
    'image_id': MRI_ids,
    'UNET_path': [root + "base_Unet_3D" + '_Results/' + MRI_id + ".nii.gz" for MRI_id in MRI_ids],
    'GAN_path':  [root + "base_GAN_3D" + '_Results/' + MRI_id + ".nii.gz" for MRI_id in MRI_ids],
    'AGAN_path': [root + "base_GAN_3D_A" + '_Results/' + MRI_id + ".nii.gz" for MRI_id in MRI_ids],
    'seg_path':  [root + MRI_id + "_seg" + ".nii" for MRI_id in MRI_ids]}

data_df = pd.DataFrame(data, columns=['image_id', 'UNET', 'GAN', 'AGAN'])

class SEG_DATA(Dataset):
    
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform

    def __getitem__(self, index):

        MRI_id = self.df['image_id'][index] 
        UNET_path = self.df['UNET_path'][index]
        GAN_path = self.df['GAN_path'][index]
        AGAN_path = self.df['AGAN_path'][index]
        LABEL_seg = self.df['seg_path'][index]


        UNET_seg = nib.load(UNET_path).get_fdata()[:].reshape(1,240,240,155)
        GAN_seg = nib.load(GAN_path).get_fdata()[:].reshape(1,240,240,155)
        AGAN_seg = nib.load(AGAN_path).get_fdata()[:].reshape(1,240,240,155)
        LABEL_seg = nib.load(LABEL_seg).get_fdata()[:].reshape(1,240,240,155)

        return UNET_seg, GAN_seg, AGAN_seg, LABEL_seg, MRI_id

    def __len__(self):
        return len(self.df)

train_split = 0.8 # Defines the ratio of train/test data.

train_size = round(len(data_df)*train_split)
test_size = round(len(data_df)*(1-train_split))

dataset_train = BRATS_DATA_CROPPED(
    df=data_df[:train_size].reset_index(drop=True),
)

dataset_test = BRATS_DATA_CROPPED(
    df=data_df[-test_size:].reset_index(drop=True),
)

dataset_total = BRATS_DATA( #used to get the final segmentations
    df=data_df[:len(data_df)].reset_index(drop=True),
)

test_loader = torch.utils.data.DataLoader(
    dataset_test,
    batch_size=1, 
    shuffle=False,
    num_workers=0,
)

tain_loader = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=1, 
    shuffle=False,
    num_workers=0,
)

full_loader = torch.utils.data.DataLoader(
    dataset_total,
    batch_size=1, 
    shuffle=False,
    num_workers=0,
)

class GeneralizedDiceLoss(nn.Module):
  """
        Generalized Dice;
        Copy from: https://github.com/wolny/pytorch-3dunet/blob/6e5a24b6438f8c631289c10638a17dea14d42051/unet3d/losses.py#L75
        paper: https://arxiv.org/pdf/1707.03237.pdf
        tf code: https://github.com/NifTK/NiftyNet/blob/dev/niftynet/layer/loss_segmentation.py#L279
  """
  def __init__(self, epsilon=1e-5, weight=None, ignore_index=None, sigmoid_normalization=True):
    super(GeneralizedDiceLoss, self).__init__()
    self.epsilon = epsilon
    self.register_buffer('weight', weight)
    self.ignore_index = ignore_index
    if sigmoid_normalization:
      self.normalization = nn.Sigmoid()
    else:
      self.normalization = nn.Softmax(dim=1)

  def forward(self, input, target):
    # get probabilities from logits
    #input = self.normalization(input)

    assert input.size() == target.size(), "'input' and 'target' must have the same shape"

    # mask ignore_index if present
    if self.ignore_index is not None:
        mask = target.clone().ne_(self.ignore_index)
        mask.requires_grad = False

        input = input * mask
        target = target * mask

    input = input.contiguous().view(-1)
    target = target.contiguous().view(-1)

    target = target.float()
    target_sum = target.sum(-1)
    class_weights = Variable(1. / (target_sum * target_sum).clamp(min=self.epsilon), requires_grad=False)

    intersect = (input * target).sum(-1) * class_weights
    if self.weight is not None:
        weight = Variable(self.weight, requires_grad=False)
        intersect = weight * intersect
    intersect = intersect.sum()

    denominator = ((input + target).sum(-1) * class_weights).sum()

    return 1. - 2. * intersect / denominator.clamp(min=self.epsilon)

checkpoint = torch.load(nobackup_models + 'base_Unet_3D' +'_checkpoint.pth.tar')
train_dice_loss_list = checkpoint['train_dice_loss_list']
test_dice_loss_list = checkpoint['test_dice_loss_list']
#print(train_dice_loss_list[-1])
#print(test_dice_loss_list[-1])

checkpoint = torch.load(nobackup_models + 'base_GAN_3D' +'_checkpoint.pth.tar')
train_dice_loss_list = checkpoint['train_dice_loss_list']
test_dice_loss_list = checkpoint['test_dice_loss_list']
#print(train_dice_loss_list[-1])
#print(test_dice_loss_list[-1])

checkpoint = torch.load(nobackup_models + 'base_Unet_3D_A' +'_checkpoint.pth.tar')
train_dice_loss_list = checkpoint['train_dice_loss_list']
test_dice_loss_list = checkpoint['test_dice_loss_list']
#print(train_dice_loss_list[-1])
#print(test_dice_loss_list[-1])

train_dice_loss = 0
for patient, (UNET_seg, GAN_seg, AGAN_seg, LABEL_seg, MRI_id) in enumerate(train_loader):
  total = UNET_seg + GAN_seg + AGAN_seg
  true_average = torch.true_divide(total, 3)
  rounded_average = torch.round(true_average)

  loss_element = criterion_Dis(rounded_average, LABEL_seg)

  train_dice_loss += loss_element
train_dice_loss = train_dice_loss/train_size

test_dice_loss = 0
for patient, (UNET_seg, GAN_seg, AGAN_seg, LABEL_seg, MRI_id) in enumerate(test_loader):
  total = UNET_seg + GAN_seg + AGAN_seg
  true_average = torch.true_divide(total, 3)
  rounded_average = torch.round(true_average)

  loss_element = criterion_Dis(rounded_average, LABEL_seg)

  test_dice_loss += loss_element
test_dice_loss = test_dice_loss/test_size

print(train_dice_loss)
print(test_dice_loss)